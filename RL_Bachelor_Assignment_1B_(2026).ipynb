{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zorftw/ITRL/blob/main/RL_Bachelor_Assignment_1B_(2026).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMrSWxqrB8sO"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "Welcome to the Reinforcement Learning Bachelor course!\n",
        "This semester consists of 4 assignments, two of which are in the form of these Google Colab Notebooks. These first two assignments are relatively small, but should provide a gentle introduction in 2 important topics underlying reinforcement learning.\n",
        "\n",
        "- A1A: Exploration\n",
        "- **A1B: Dynamic Programming**\n",
        "- A1: Model-Free Reinforcement Learning\n",
        "- A2: Model-Based Reinforcement Learning\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCdlZ6_Y3R2H"
      },
      "source": [
        "## Assignment Requirements\n",
        "\n",
        "The first two notebook assignments require you to read the Colab carefully, complete some (small) coding blocks within the notebook and write a report.\n",
        "\n",
        "Throughout each notebook you will find questions that should provide inspiration for discussion points in your report. At the end of the notebook there will also be a summary of how the report should look like and what it should include.\n",
        "\n",
        "#### What to hand in\n",
        "\n",
        "1. This notebook, with code blocks completed.\n",
        "    - Hand in the notebook **with** the results (primarily the plots) under each code cell; so do not delete the cell output before downloading the file.\n",
        "    - Download this notebook file via \"*File > Download > .ipynb Download*\".\n",
        "2. A small 1-2 page double-column report, created in latex using the template linked on [the website](https://irl.liacs.nl/assignments).\n",
        "\n",
        "Hand in both the `ipynb` and the `pdf` as standalone files (**not as a ZIP file**) on brightspace before the due date.\n",
        "\n",
        "Both assignment 1A and 1B should be accompanied by a standalone report (max 3 pages vor 1A, 2 pages for 1B). These can be handed in as seperate pdfs, or merged into a single file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtva1zFfFtnn"
      },
      "source": [
        "# Assingment 1B - Dynamic Programming\n",
        "\n",
        "This second assignment focusses on on dynamic programming (DP), which is a bridging method between planning and reinforcement learning (RL). Whereas RL learns via sampling, DP learns via (perfect) planning. As such, DP assumes full access to a \"model\" of the environment i.e. we can get $p(s'|s,a)$ and $r(s,a,s')$ for any state $s$ and action $a$ (In case this is not at all familiar to you, please check out week's 3 slides on MDPs).\n",
        "\n",
        "Due to the nature of DP, there is no **exploration** involved in this assignment. However, we will still assume you have already completed the last notebook and understand all the code that was provided there."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GVP4NbM4bce0",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Code Initialization\n",
        "#@markdown <u>Please run this cell block before continuing. This will create the environment class along with some helpfull visualization tools. You can inspect the code, but it is not required to fully understand it.</u>\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Rectangle,Circle,Arrow\n",
        "from IPython import display\n",
        "from time import sleep\n",
        "\n",
        "RENDER_SCALE = 0.85\n",
        "STEP_PAUSE=0.2\n",
        "\n",
        "class WindyGridworld:\n",
        "    ''' Stochastic version of WindyGridworld\n",
        "        (Sutton & Barto, Example 6.5 at page 130, see http://incompleteideas.net/book/RLbook2020.pdf)\n",
        "        Compared to the book version, the vertical wind is now stochastic, and only blows 80% of the times\n",
        "    '''\n",
        "\n",
        "    def __init__(self, initialize_model=True, reward_per_step=-1.0, wind_blows_proportion = 1.0):\n",
        "        self.height = 6\n",
        "        self.width = 7\n",
        "        self.start_location = (0,3)\n",
        "        self.goal_location = (4,3)\n",
        "        self.reward_per_step = reward_per_step\n",
        "        self.goal_reward = 35\n",
        "        self.shape = (self.width, self.height)\n",
        "        self.n_states = self.height * self.width\n",
        "        self.n_actions = 4\n",
        "        self.winds = (0,0,1,2,2,1,0)\n",
        "        self.wind_blows_proportion = wind_blows_proportion\n",
        "        self.action_effects = {\n",
        "                0: (0, 1),  # up\n",
        "                1: (1, 0),   # right\n",
        "                2: (0, -1),   # down\n",
        "                3: (-1, 0),  # left\n",
        "                }\n",
        "        self.initialize_model = initialize_model\n",
        "        if initialize_model:\n",
        "            self._construct_model()\n",
        "        self.fig = None\n",
        "        self.Q_labels = None\n",
        "        self.arrows = None\n",
        "        self.reset() # set agent to the start location\n",
        "\n",
        "    def reset(self):\n",
        "        ''' set the agent back to the start location '''\n",
        "        self.agent_location = np.array(list(self.start_location))\n",
        "        s = self._location_to_state(self.agent_location)\n",
        "        return s\n",
        "\n",
        "    def step(self,a):\n",
        "        ''' Forward the environment based on action a, really affecting the agent location\n",
        "        Returns the next state, the obtained reward, and a boolean whether the environment terminated '''\n",
        "        self.agent_location += self.action_effects[a] # effect of action\n",
        "        self.agent_location = np.clip(self.agent_location,(0,0),np.array(self.shape)-1) # bound within grid\n",
        "        if np.random.rand() < self.wind_blows_proportion: # apply effect of wind\n",
        "            self.agent_location[1] += self.winds[self.agent_location[0]] # effect of wind\n",
        "        self.agent_location = np.clip(self.agent_location,(0,0),np.array(self.shape)-1) # bound within grid\n",
        "        s_next = self._location_to_state(self.agent_location)\n",
        "\n",
        "        # Check reward and termination\n",
        "        if np.all(self.agent_location == self.goal_location):\n",
        "            done = True\n",
        "            r = self.goal_reward\n",
        "        else:\n",
        "            done = False\n",
        "            r = self.reward_per_step\n",
        "\n",
        "        return s_next, r, done\n",
        "\n",
        "    def model(self,s,a):\n",
        "        ''' Returns vectors p(s'|s,a) and r(s,a,s') for given s and a.\n",
        "        Only simulates, does not affect the current agent location '''\n",
        "        if self.initialize_model:\n",
        "            return self.p_sas[s,a], self.r_sas[s,a]\n",
        "        else:\n",
        "            raise ValueError(\"set initialize_model=True when creating Environment\")\n",
        "\n",
        "\n",
        "    def render(self,Q_sa=None,plot_optimal_policy=False,step_pause=None):\n",
        "        ''' Plot the environment\n",
        "        if Q_sa is provided, it will also plot the Q(s,a) values for each action in each state\n",
        "        if plot_optimal_policy=True, it will additionally add an arrow in each state to indicate the greedy action '''\n",
        "        # Initialize figure\n",
        "        # if self.fig == None:\n",
        "        self._initialize_plot()\n",
        "        if step_pause == None:\n",
        "            step_pause = STEP_PAUSE\n",
        "\n",
        "        # Add Q-values to plot\n",
        "        if Q_sa is not None:\n",
        "            # Initialize labels\n",
        "            # if self.Q_labels is None:\n",
        "            self._initialize_Q_labels()\n",
        "            # Set correct values of labels\n",
        "            for state in range(self.n_states):\n",
        "                for action in range(self.n_actions):\n",
        "                    self.Q_labels[state][action].set_text(np.round(Q_sa[state,action],1))\n",
        "\n",
        "        # Add arrows of optimal policy\n",
        "        if plot_optimal_policy and Q_sa is not None:\n",
        "            self._plot_arrows(Q_sa)\n",
        "\n",
        "        # Update agent location\n",
        "        self.agent_circle.center = self.agent_location+0.5\n",
        "\n",
        "        # Draw figure\n",
        "        display.display(plt.gcf())\n",
        "        display.clear_output(wait=True)\n",
        "        sleep(step_pause)\n",
        "\n",
        "    def _state_to_location(self,state):\n",
        "        ''' bring a state index to an (x,y) location of the agent '''\n",
        "        return np.array(np.unravel_index(state,self.shape))\n",
        "\n",
        "    def _location_to_state(self,location):\n",
        "        ''' bring an (x,y) location of the agent to a state index '''\n",
        "        return np.ravel_multi_index(location,self.shape)\n",
        "\n",
        "    def _construct_model(self):\n",
        "        ''' Constructs full p(s'|s,a) and r(s,a,s') arrays\n",
        "            Stores these in self.p_sas and self.r_sas '''\n",
        "\n",
        "        # Initialize transition and reward functions\n",
        "        p_sas = np.zeros((self.n_states,self.n_actions,self.n_states))\n",
        "        r_sas = np.zeros((self.n_states,self.n_actions,self.n_states)) + self.reward_per_step\n",
        "\n",
        "        for s in range(self.n_states):\n",
        "            for a in range(self.n_actions):\n",
        "                s_location = self._state_to_location(s)\n",
        "\n",
        "                # if s is terminal state make it a self-loop without rewards\n",
        "                if np.all(s_location == self.goal_location):\n",
        "                    goal_state = self._location_to_state(self.goal_location)\n",
        "                    p_sas[s,a,goal_state] = 1.0\n",
        "                    r_sas[s,a,] = np.zeros(self.n_states)\n",
        "                    continue\n",
        "\n",
        "                # check what happens if the wind blows:\n",
        "                next_location_with_wind = np.copy(s_location)\n",
        "                next_location_with_wind += self.action_effects[a] # effect of action\n",
        "                next_location_with_wind = np.clip(next_location_with_wind,(0,0),np.array(self.shape)-1) # bound within grid\n",
        "                next_location_with_wind[1] += self.winds[next_location_with_wind[0]] # Apply effect of wind\n",
        "                next_location_with_wind = np.clip(next_location_with_wind,(0,0),np.array(self.shape)-1) # bound within grid\n",
        "                next_state_with_wind = self._location_to_state(next_location_with_wind)\n",
        "\n",
        "                # Update p_sas and r_sas\n",
        "                p_sas[s,a,next_state_with_wind] += self.wind_blows_proportion\n",
        "                if np.all(next_location_with_wind == self.goal_location):\n",
        "                    r_sas[s,a,next_state_with_wind]  = self.goal_reward\n",
        "\n",
        "                # check what happens if the wind blows:\n",
        "                next_location_without_wind = np.copy(s_location)\n",
        "                next_location_without_wind += self.action_effects[a] # effect of action\n",
        "                next_location_without_wind = np.clip(next_location_without_wind,(0,0),np.array(self.shape)-1) # bound within grid\n",
        "                next_state_without_wind = self._location_to_state(next_location_without_wind)\n",
        "\n",
        "                # Update p_sas and r_sas\n",
        "                p_sas[s,a,next_state_without_wind] += (1-self.wind_blows_proportion)\n",
        "                if np.all(next_location_without_wind == self.goal_location):\n",
        "                    r_sas[s,a,next_state_without_wind]  = self.goal_reward\n",
        "\n",
        "        self.p_sas = p_sas\n",
        "        self.r_sas = r_sas\n",
        "        return\n",
        "\n",
        "    def _initialize_plot(self):\n",
        "        self.fig, self.ax = plt.subplots(figsize=(self.width * RENDER_SCALE, self.height * RENDER_SCALE))\n",
        "        self.ax.set_xlim([0,self.width])\n",
        "        self.ax.set_ylim([0,self.height])\n",
        "        self.ax.axes.xaxis.set_visible(False)\n",
        "        self.ax.axes.yaxis.set_visible(False)\n",
        "\n",
        "        for x in range(self.width):\n",
        "            for y in range(self.height):\n",
        "                self.ax.add_patch(Rectangle((x, y),1,1, linewidth=0, facecolor='k',alpha=self.winds[x]/4))\n",
        "                self.ax.add_patch(Rectangle((x, y),1,1, linewidth=0.5, edgecolor='k', fill=False))\n",
        "\n",
        "        self.ax.axvline(0,0,self.height,linewidth=5,c='k')\n",
        "        self.ax.axvline(self.width,0,self.height,linewidth=5,c='k')\n",
        "        self.ax.axhline(0,0,self.width,linewidth=5,c='k')\n",
        "        self.ax.axhline(self.height,0,self.width,linewidth=5,c='k')\n",
        "\n",
        "\n",
        "        # Indicate start and goal state\n",
        "        self.ax.add_patch(Rectangle(self.start_location,1.,1., linewidth=0, facecolor='r',alpha=0.2))\n",
        "        self.ax.add_patch(Rectangle(self.goal_location,1.,1., linewidth=0, facecolor='g',alpha=0.2))\n",
        "        self.ax.text(self.start_location[0]+0.05,self.start_location[1]+0.75, 'S', fontsize=20, c='r')\n",
        "        self.ax.text(self.goal_location[0]+0.05,self.goal_location[1]+0.75, 'G', fontsize=20, c='g')\n",
        "\n",
        "\n",
        "        # Add agent\n",
        "        self.agent_circle = Circle(self.agent_location+0.5,0.3)\n",
        "        self.ax.add_patch(self.agent_circle)\n",
        "\n",
        "    def _initialize_Q_labels(self):\n",
        "        self.Q_labels = []\n",
        "        for state in range(self.n_states):\n",
        "            state_location = self._state_to_location(state)\n",
        "            self.Q_labels.append([])\n",
        "            for action in range(self.n_actions):\n",
        "                plot_location = np.array(state_location) + 0.38 + 0.30 * np.array(self.action_effects[action])\n",
        "                next_label = self.ax.text(plot_location[0],plot_location[1]+0.03,0.0,fontsize=8)\n",
        "                self.Q_labels[state].append(next_label)\n",
        "\n",
        "    def _plot_arrows(self,Q_sa):\n",
        "        if self.arrows is not None:\n",
        "            for arrow in self.arrows:\n",
        "                arrow.remove() # Clear all previous arrows\n",
        "        self.arrows=[]\n",
        "        for state in range(self.n_states):\n",
        "            plot_location = np.array(self._state_to_location(state)) + 0.5\n",
        "            max_actions = full_argmax(Q_sa[state])\n",
        "            for max_action in max_actions:\n",
        "                new_arrow = arrow = Arrow(plot_location[0],plot_location[1],self.action_effects[max_action][0]*0.2,\n",
        "                                          self.action_effects[max_action][1]*0.2, width=0.05,color='k')\n",
        "                ax_arrow = self.ax.add_patch(new_arrow)\n",
        "                self.arrows.append(ax_arrow)\n",
        "\n",
        "def full_argmax(x):\n",
        "    ''' Own variant of np.argmax, since np.argmax only returns the first occurence of the max '''\n",
        "    return np.where(x == np.max(x))[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hLD44QUbPaB"
      },
      "source": [
        "## The Environment\n",
        "\n",
        "You will study DP on the *Windy GridWorld*, an adapted version of Example 6.5 (page 130) in *Reinforcement Learning: An Introduction (second edition)* by Sutton and Barto. Provided you have ran the cell above, you can visualize the environment by running the cell below.\n",
        "\n",
        "The environment consists of a 7x6 grid, where at each cell we can move up, down, left or right. We start at location (0,3) (we start indexing at 0, as is done in Python as well), indicated in the figure by ‘S’. Our goal is to move to location (4,3), indicated by ‘G’. However, a special feature of the environment is that there is a vertical wind. In columns 2 and 5, we are pushed one additional step up, while in columns 3 and 4, we move up two additional steps. The reward of the agent at each step is -1, while reaching the goal gives a reward of +35, and terminates the episode."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 429
        },
        "id": "nbiBvapaips_",
        "outputId": "2f314d02-1a03-4613-e0ab-cfdd483032af"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 595x510 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "env = WindyGridworld()\n",
        "s = env.reset()\n",
        "env.render(Q_sa=None,plot_optimal_policy=False,step_pause=1.) # display the environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_45w4aw646GU"
      },
      "source": [
        "## The Agent\n",
        "\n",
        "Below, we have implemented a `PolicyIterationAgent`, a dynamic programming agent. This implementation is quite explicit, and you will have to create a simplified improvement later. For now, this verbose implementation allows us to highlight some important characteristics in any kind of agent.\n",
        "\n",
        "### The Policy and Selecting Actions\n",
        "\n",
        "We have introduced **policy $π$** in the previous assignment and explained how its usually implemented implicitely. Here, to better highlight what is going on, we will be a bit more explicit and include the actual action distribution as part of the agent. The policy (`self.policy`) is therefor represented as a 2D array where each row represents a state and gives the probabilities of selecting each action in that state (so the rows should sum up to 1.0 (100%)).\n",
        "If our policy is *greedy*, we have a 100% chance of selecting a single action in each state, and 0% chance of selecting other actions.\n",
        "In `select_action()`, we now sample from our policy.\n",
        "\n",
        "Our goal is still to find the optimal policy ($π^*$), so the training loop now explicitely updates the policy via `update_policy()`. Here, we change the probabilities to be greedy with respect to our Q-values. This should give us the best possible improvement in our policy, given that our Q-values are good estimates.\n",
        "\n",
        "### The Q-values\n",
        "\n",
        "Similar to our bandit agents of the previous assignment, our new agent will store Q-values for each action. Because we are now concerned with states, the Q-arrray is transformed into a 2D Q-table.\n",
        "Furthermore, it is important to realize that Q-values are not just the expected reward from a given action in a state. Instead, they represent the expected reward from a given action in a state **and** the (discounted) rewards we obtain by thereafter following policy $π$ (We will get into the discounted part later). As such, Q-values (or state-values $V_π$, see slides) are **always** conditioned on a policy -- hence $Q_π$.\n",
        "\n",
        "A mathematical formulation for $Q_π$ is given by the bellman-equations, which are the foundation for updating $Q_π$ in dynamic programming:\n",
        "$$\n",
        "Q_π(s,a) \\gets \\sum_{s',r}p(s',r|s,a)[r+ \\gamma \\sum_{a'}p(a'|π) \\cdot Q(s',a')].\n",
        "$$\n",
        "\n",
        "Carefully study the equation above (and/or `update_Q` function below). For now, assume $\\gamma = 1$. You will notice that $Q_π(s,a)$ is partly defined by $Q_π(s',a')$ (the next state and action). As a result, $Q_π(s',a')$ needs to be a correct estimate first.\n",
        "We can achieve this by repeatedly applying the function on all state-action pairs.\n",
        "If you run the below code block, this is visualized. The training loop will loop over each state-action pair and update the Q-values (starting at the bottom left) and this process is repeated multiple times. We will discuss the training loop below the code block.\n",
        "***\n",
        "Stop running the code cell below once you've seen enough, as running the full cycle will take a long time. Also, don't be alarmed of the image flashing on every update. This is due to how the colab notebook deals with dynamic visualizations.\n",
        "***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CuEOHw_1mobe"
      },
      "outputs": [],
      "source": [
        "class PolicyIterationAgent():\n",
        "    def __init__(\n",
        "            self,\n",
        "            n_states,\n",
        "            n_actions,\n",
        "            transition_model, # DP agents require a perfect model\n",
        "            reward_model, # DP agents require a perfect model\n",
        "        ):\n",
        "        self.gamma = 1.0\n",
        "        self.n_states = n_states\n",
        "        self.n_actions = n_actions\n",
        "        self.Q = np.zeros((n_states, n_actions)) # Initialize our estimates at 0\n",
        "        self.policy = np.ones((n_states, n_actions)) / n_actions # Initialize fully random policy\n",
        "\n",
        "        self.p_sas = transition_model\n",
        "        self.r_sas = reward_model\n",
        "\n",
        "    def select_action(self, s):\n",
        "        \"\"\" Sample an action for state s from our policy \"\"\"\n",
        "        return np.random.choice(self.n_actions, p=self.policy[s])\n",
        "\n",
        "    def update_Q(self, s, a):\n",
        "        \"\"\" Update our estimated sum of rewards given the current policy (Q) \"\"\"\n",
        "\n",
        "        self.Q[s, a] = np.sum(\n",
        "            self.p_sas[s, a] * (\n",
        "                self.r_sas[s, a] + self.gamma * np.sum(\n",
        "                    self.policy * self.Q , axis=1\n",
        "                )\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def update_policy(self):\n",
        "        \"\"\" Update our policy by choosing the greedy action for each state \"\"\"\n",
        "        for s in range(self.n_states):\n",
        "            best_action = np.argmax(self.Q[s]) # <-- takes the first best action\n",
        "            self.policy[s] = np.zeros(self.n_actions)\n",
        "            self.policy[s, best_action] = 1.0\n",
        "\n",
        "            # We could also split the percentages over all best actions (in case of ties):\n",
        "            # best_actions = np.argwhere(self.Q[s] == np.max(self.Q[s])).flatten()\n",
        "            # self.policy[s] = np.isin(np.arange(self.n_actions), best_actions).astype(int) / len(best_actions)\n",
        "\n",
        "    def train(self, threshold=0.01, eval_iterations=np.inf, render_fn=None, verbose=0):\n",
        "        \"\"\" Trains the agent via Policy Iteration \"\"\"\n",
        "\n",
        "        if eval_iterations == np.inf:\n",
        "            eval_iterations = 9e10 # cannot loop over inf float, so we just change it to something big\n",
        "\n",
        "        while True:\n",
        "            current_policy = self.policy.copy() # To check if the policy is stable\n",
        "            for i in range(int(eval_iterations)):\n",
        "                if verbose > 0:\n",
        "                    print(\"Policy Evaluation iteration: \", i + 1)\n",
        "                current_Q = self.Q.copy() # To check if Q is stable\n",
        "                for s in range(self.n_states):\n",
        "                    for a in range(self.n_actions):\n",
        "                        self.update_Q(s, a)\n",
        "                        if render_fn is not None:\n",
        "                            render_fn(Q_sa=self.Q)\n",
        "                delta = np.max(np.abs(current_Q - self.Q))\n",
        "                if delta < threshold:\n",
        "                    break\n",
        "            self.update_policy()\n",
        "            policy_delta = np.max(np.abs(current_policy - self.policy))\n",
        "            if policy_delta < threshold:\n",
        "                break\n",
        "\n",
        "STEP_PAUSE = 0.5\n",
        "env = WindyGridworld()\n",
        "agent = PolicyIterationAgent(\n",
        "    n_states=env.n_states,\n",
        "    n_actions=env.n_actions,\n",
        "    transition_model=env.p_sas,\n",
        "    reward_model=env.r_sas\n",
        ")\n",
        "agent.train(render_fn=env.render)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phpRMBIJyHGt"
      },
      "source": [
        "## Policy Iteration - Train Loop\n",
        "\n",
        "We now know how to update $Q_π$ for any $(s,a)$-pair and how we can use $Q_π$ to improve policy π. We also know that to calculate $Q_π$ -- we have to apply the update rule multiple times to every state-action pair. In fact, we have to apply the update rule an infinite amount of times to calculate it exactly.\n",
        "Since we often lack an infinite amount of time, we typically set some threshold value and stop updating when the changes in $Q_π$ become smaller than this value. Knowing all this, you should be able to understand why the below psuodocode (and the implemented training loop above) works.\n",
        "\n",
        "<img width=\"75%\" src='https://drive.usercontent.google.com/download?id=1-QTdMgiewixcj2ad_9IawadDxXErtVLr' />\n",
        "\n",
        "Run the code cell below to run the full training loop (without the intermediate plotting). This should take roughly a minute or two and end with a nice plot showing the converged Q-values along with the best action to take in each state.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DdBZW19ygJDT"
      },
      "outputs": [],
      "source": [
        "env = WindyGridworld()\n",
        "agent = PolicyIterationAgent(\n",
        "    n_states=env.n_states,\n",
        "    n_actions=env.n_actions,\n",
        "    transition_model=env.p_sas,\n",
        "    reward_model=env.r_sas\n",
        ")\n",
        "agent.train(verbose=1)\n",
        "env.render(Q_sa=agent.Q, plot_optimal_policy=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LadItnbvg-LS"
      },
      "source": [
        "Just to recap, the training loop we just discribed just consists of two phases:\n",
        "\n",
        "1. **Policy Evaluation**: We obtain the (near) exact values for $Q_π$ by iteratively applying the bellman-equation for $Q$ on every state-action pair;\n",
        "2. **Policy Improvement**: We change the policy to behave greedy with respect with the new $Q_π$.\n",
        "\n",
        "This two-step process is known as **policy iteration** and is garentueed to converge on the optimal policy in **any** finite MDP.\n",
        "You will notice though, that it took almost two minutes to solve this very simple problem... This time was mostly spend in the **policy evaluation** phase. After it found correct $Q_π$, the policy updated to the optimal policy almost instantly.\n",
        "\n",
        "Luckily, we can make a shortcut in the training process. Try rerunning the training loop below with a few different (low) values for `eval_iterations`. The **(1) policy evaluation** phase will now quit early, before the threshold is met."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D2VVjxDfj6Hq"
      },
      "outputs": [],
      "source": [
        "eval_iterations =     # Try some low values -- go as low as 1\n",
        "\n",
        "env = WindyGridworld()\n",
        "agent = PolicyIterationAgent(\n",
        "    n_states=env.n_states,\n",
        "    n_actions=env.n_actions,\n",
        "    transition_model=env.p_sas,\n",
        "    reward_model=env.r_sas\n",
        ")\n",
        "agent.train(verbose=1, eval_iterations=eval_iterations)\n",
        "env.render(Q_sa=agent.Q, plot_optimal_policy=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSNsn7RfkLtw"
      },
      "source": [
        "As you will see, we converged on the same policy **far** quicker. It turns out that in order to do a **policy improvement**, we do not need $Q_π$ to have converged at all. All we need, is for our Q-values to have *moved* to a better estimate of $Q_\\pi$ -- which already happened after applying the bellman equation once. In fact, in the training loop code above, we can move the `self.update_policy()` line right behind the `self.update_Q()`. Of course, we require a few more **policy improvement** steps in our new setup, but the entire loop is often much quicker.\n",
        "\n",
        "We use the term **generalized policy iteration (GPI)** for any two-step process of policy evaluation and policy improvement with arbitrary granularities. Importantly, **ALL** current reinforcement learning algorithms are part of the GPI class algorithms -- including state-of-the-art deep learning based methods used to create AlphaGo or to align ChatGPT. In most algorithms though, parts of the process are implicit (like updating the policy).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhYJpsRLp9n0"
      },
      "source": [
        "## Value Iteration\n",
        "\n",
        "Knowing that we can skip a lot of policy evaluation steps, we can create a more efficient and simpler DP algorithm known as **Value Iteration**. It is up to you to construct this agent below and report on its performance in your report.\n",
        "\n",
        "You can find the psuodo-code for value iteration below. Note that the `ValueIterationAgent` does not require an explicit policy as it can directly select an action based on the current Q-values. In turn, line 9 occurs implicitely when correctly implementing the `select_action` function.\n",
        "\n",
        "<img width=\"75%\" src='https://drive.usercontent.google.com/download?id=1-RrTMroIAwc4mm53u3A3MK9fe1aQO7Uc' />"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Igp_cR3UtfSu"
      },
      "outputs": [],
      "source": [
        "class ValueIterationAgent():\n",
        "    def __init__(\n",
        "            self,\n",
        "            n_states,\n",
        "            n_actions,\n",
        "            transition_model, # DP agents require a perfect model\n",
        "            reward_model, # DP agents require a perfect model\n",
        "            gamma=1.0\n",
        "        ):\n",
        "        self.n_states = n_states\n",
        "        self.n_actions = n_actions\n",
        "        self.Q = np.zeros((n_states, n_actions)) # Initialize our estimates at 0\n",
        "        self.gamma = gamma\n",
        "\n",
        "        self.p_sas = transition_model\n",
        "        self.r_sas = reward_model\n",
        "\n",
        "    def select_action(self, s):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def update_Q(self, s, a):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def train(self, threshold=0.01, render_fn=None, verbose=0):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "\n",
        "env = WindyGridworld()\n",
        "agent = ValueIterationAgent(\n",
        "    n_states=env.n_states,\n",
        "    n_actions=env.n_actions,\n",
        "    transition_model=env.p_sas,\n",
        "    reward_model=env.r_sas\n",
        ")\n",
        "agent.train()\n",
        "env.render(Q_sa=agent.Q, plot_optimal_policy=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8M7-lwwzP-Q"
      },
      "source": [
        "## Discounting\n",
        "\n",
        "In our update rules so far we have set $\\gamma=1$. $\\gamma$ is known as the discount factor. With $γ < 1$ we weigh future rewards less then current rewards. For example, we may simulate an agent that favors gaining money now over gaining the same amount of money in the future. However, there are other reasons we include $γ$, which you should discuss in your report.\n",
        "\n",
        "Below we instantiate a new environment where each step no longer gives a penalty of -1. Run this cell and observe the result. Then, change gamma in your `ValueIterationAgent` so the agent can solve the task again. Reason about this in your report."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F0Mh2zXZzPgS"
      },
      "outputs": [],
      "source": [
        "env = WindyGridworld(reward_per_step=0.0)\n",
        "agent = ValueIterationAgent(\n",
        "    n_states=env.n_states,\n",
        "    n_actions=env.n_actions,\n",
        "    transition_model=env.p_sas,\n",
        "    reward_model=env.r_sas,\n",
        "    gamma=1.0\n",
        ")\n",
        "agent.train()\n",
        "env.render(Q_sa=agent.Q, plot_optimal_policy=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stochasticity\n",
        "\n",
        "So far, our WindyGridWorld has behaved very predictable as the wind blows 100% of the times we expect it to. Now, we alter the environment such that the wind only blows a certain percentage of times. Our environment is now **stochastic** and our agent has to incorporate this into it's estimated Q-values. Luckily, in case update function is implemented correctly, your `ValueIterationAgent` should have no issue finding a new optimal solution.\n",
        "\n",
        "Experiment with different levels of stochasticity below. In your report, explain why your agent can deal with stochasticity without any changes -- what component of the bellman equation handles this."
      ],
      "metadata": {
        "id": "z18ymI08B36M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment with different values:\n",
        "WIND_BLOWS_PROPORTION=0.75\n",
        "env = WindyGridworld(wind_blows_proportion=WIND_BLOWS_PROPORTION)\n",
        "agent = ValueIterationAgent(\n",
        "    n_states=env.n_states,\n",
        "    n_actions=env.n_actions,\n",
        "    transition_model=env.p_sas,\n",
        "    reward_model=env.r_sas\n",
        ")\n",
        "agent.train()\n",
        "env.render(Q_sa=agent.Q, plot_optimal_policy=True)"
      ],
      "metadata": {
        "id": "2S1I89XCFdjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqzH0bxh2F5J"
      },
      "source": [
        "# Report Summary\n",
        "\n",
        "Along with this notebook, you are to hand in a 1-2 page two-column report which summarizes this notebook, and discusses the different algorithms and results you obtained.\n",
        "\n",
        "**General guidelines:**\n",
        "- Assignment reports should be a standalone work. Instead of reading a solution to an assignment, the report should be structured and feel like a short academic paper. [The website](https://irl.liacs.nl/assignments) lists some example reports.\n",
        "- Make sure you use the template linked on [the website](https://irl.liacs.nl/assignments)\n",
        "- You can omit the abstract, but make sure you have a strong introduction: What is the paper about, why is that important, how are we tackling that problem etc.\n",
        "- Going over the page limit (excluding references) will subtract 1 point per half page.\n",
        "- Make sure to include some steps for future work. For example, can certain elements be combined? Be precise on how these next steps would look like.\n",
        "- When displaying and discussing results, never only type down what we can all see in the figure/table. Instead, reason about your results. Explain why you think we observe them and what we can deduce.\n",
        "- When an environment is involved, make sure this is (briefly) introduced to the reader.\n",
        "\n",
        "**These items should be discussed in your report:**\n",
        "- What is an MDP\n",
        "    - Discuss its elements and various reasons we include γ (gamma) (You can use the setting with `reward_per_step=0` environment as an example here).\n",
        "- Explain GPI\n",
        "- What is dynamic programming and what is used for\n",
        "    - what are its advantages and disadvantages\n",
        "    - Relate DP back to GPI\n",
        "- You can use Policy Iteration and Value iteration as examples in your report for the explanations of DP and/or GPI, but you are not required to explain both algorithms in detail. I.e. explain them intuitively, but do not provide full psuodocode.\n",
        "- Explain the bellman/bellman optimality equation and its components.\n",
        "- Include at least one image of the WindyGridWorld with the the fully trained Q-values and optimal path + one image with stochastic wind (0.75) Discuss these plots."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}